{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11a920ab-b270-4adf-b0d5-6abedfa8e3cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Assignment 1: Bronze Layer Example - CVE 2024 Data Ingestion\n",
    "**DIC 587 - Data Intensive Computing - Fall 2025**\n",
    "\n",
    "This notebook implements the Bronze layer of our medallion architecture:\n",
    "- Downloads CVEProject/cvelistV5 repository \n",
    "- Filters to 2024 vulnerabilities only (~40,000 records)\n",
    "- Stores raw JSON as Delta tables with ACID guarantees\n",
    "- Demonstrates streaming data ingestion patterns\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand Bronze layer concepts (raw data preservation)\n",
    "- Practice JSON schema-on-read with PySpark\n",
    "- Learn Delta Lake table registration\n",
    "- Handle large-scale data downloads programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972e6646-7e00-4462-8995-d291f5798fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Environment Setup and Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37df5b5-3945-4c95-b832-7bd9a0d5f512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 2024 CVE data (~40,000 records)\nBronze location: /Volumes/workspace/default/assignment1/cve/bronze\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Configuration\n",
    "USE_OFFLINE = False  # Set True if you manually uploaded /FileStore/cvelistV5.zip\n",
    "year = 2024   # Focus on 2024 CVEs only\n",
    "\n",
    "# Paths - using /tmp and /FileStore to avoid Community Edition I/O errors\n",
    "base_dir = \"/tmp/cve/\"\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"default\"\n",
    "DELTA_BRONZE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/assignment1/cve/bronze\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "os.makedirs(DELTA_BRONZE_PATH, exist_ok=True)\n",
    "\n",
    "# for registering sql table\n",
    "schema_name = \"cve_bronze\"\n",
    "table_name = \"records\"\n",
    "\n",
    "print(f\"Target: {year} CVE data (~40,000 records)\")\n",
    "print(f\"Bronze location: {DELTA_BRONZE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb4dcc8-b02a-43d0-aff1-cfcf1e68ca22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Download CVE Repository\n",
    "\n",
    "This uploads the entire CVE catalog into a databricks catalog.  You need to figure out how to filter it down to the 2024 records.  You also need to do the other medallion letters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0b5898-59d5-429c-a081-5f3f2a47e344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CVE repository...\nDownloading from: https://github.com/CVEProject/cvelistV5/archive/refs/heads/main.zip\nDownloaded 524,599,814 bytes\nExtracting ZIP archive...\nExtraction complete\n"
     ]
    }
   ],
   "source": [
    "# Download CVE repository\n",
    "print(\"Downloading CVE repository...\")\n",
    "\n",
    "# Configuration\n",
    "volume_root = \"/Volumes/workspace/default/assignment1\"\n",
    "\n",
    "# Your existing download/extract pattern\n",
    "zip_dest = f\"{base_dir}cvelistV5.zip\"\n",
    "extract_dir = f\"{base_dir}cvelistV5-main\"\n",
    "\n",
    "# Create local temp directory\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Clean previous extracts\n",
    "if os.path.exists(extract_dir):\n",
    "    shutil.rmtree(extract_dir, ignore_errors=True)\n",
    "\n",
    "# Download the repository\n",
    "zip_url = \"https://github.com/CVEProject/cvelistV5/archive/refs/heads/main.zip\"\n",
    "print(f\"Downloading from: {zip_url}\")\n",
    "\n",
    "with urllib.request.urlopen(zip_url) as response:\n",
    "    data = response.read()\n",
    "    \n",
    "with open(zip_dest, \"wb\") as f:\n",
    "    f.write(data)\n",
    "\n",
    "print(f\"Downloaded {len(data):,} bytes\")\n",
    "\n",
    "# Extract the ZIP\n",
    "print(\"Extracting ZIP archive...\")\n",
    "with zipfile.ZipFile(zip_dest) as z:\n",
    "    z.extractall(base_dir)\n",
    "\n",
    "print(\"Extraction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8bcfab-9d65-4be7-835f-dfcc9293d2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing {year} CVEs...\nScanning directory: /tmp/cve/cvelistV5-main/cves/2024\nProcessed 1000 CVE-2024 files...\nProcessed 2000 CVE-2024 files...\nProcessed 3000 CVE-2024 files...\nProcessed 4000 CVE-2024 files...\nProcessed 5000 CVE-2024 files...\nProcessed 6000 CVE-2024 files...\nProcessed 7000 CVE-2024 files...\nProcessed 8000 CVE-2024 files...\nProcessed 9000 CVE-2024 files...\nProcessed 10000 CVE-2024 files...\nProcessed 11000 CVE-2024 files...\nProcessed 12000 CVE-2024 files...\nProcessed 13000 CVE-2024 files...\nProcessed 14000 CVE-2024 files...\nProcessed 15000 CVE-2024 files...\nProcessed 16000 CVE-2024 files...\nProcessed 17000 CVE-2024 files...\nProcessed 18000 CVE-2024 files...\nProcessed 19000 CVE-2024 files...\nProcessed 20000 CVE-2024 files...\nProcessed 21000 CVE-2024 files...\nProcessed 22000 CVE-2024 files...\nProcessed 23000 CVE-2024 files...\nProcessed 24000 CVE-2024 files...\nProcessed 25000 CVE-2024 files...\nProcessed 26000 CVE-2024 files...\nProcessed 27000 CVE-2024 files...\nProcessed 28000 CVE-2024 files...\nProcessed 29000 CVE-2024 files...\nProcessed 30000 CVE-2024 files...\nProcessed 31000 CVE-2024 files...\nProcessed 32000 CVE-2024 files...\nProcessed 33000 CVE-2024 files...\nProcessed 34000 CVE-2024 files...\nProcessed 35000 CVE-2024 files...\nProcessed 36000 CVE-2024 files...\nProcessed 37000 CVE-2024 files...\nProcessed 38000 CVE-2024 files...\nCollected 38753 CVEs from 2024\nTotal 2024 CVEs: 38753\nConverting 38753 CVEs to DataFrame...\nTotal 2024 CVE records found: 38,753\n"
     ]
    }
   ],
   "source": [
    "# Process 2024 CVEs\n",
    "print(\"Processing {year} CVEs...\")\n",
    "\n",
    "def process_year_cves(year, max_files=100000):\n",
    "    \"\"\"Process CVEs for a specific year, limiting to max_files\"\"\"\n",
    "    \n",
    "    cve_year_dir = f\"{extract_dir}/cves/{year}\"\n",
    "    json_files = []\n",
    "    \n",
    "    print(f\"Scanning directory: {cve_year_dir}\")\n",
    "    \n",
    "    # read JSON files into list, filtering by target year\n",
    "    if os.path.exists(cve_year_dir):\n",
    "        file_count = 0\n",
    "        for root, dirs, files in os.walk(cve_year_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.json') and f'CVE-{year}-' in file and file_count < max_files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        # Read and validate JSON\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            cve_data = json.loads(content)\n",
    "                            json_files.append(cve_data)\n",
    "                            \n",
    "                        file_count += 1\n",
    "                        \n",
    "                        if file_count % 1000 == 0:\n",
    "                            print(f\"Processed {file_count} CVE-{year} files...\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipped {file}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "        print(f\"Collected {len(json_files)} CVEs from {year}\")\n",
    "        return json_files\n",
    "    else:\n",
    "        print(f\"Directory not found: {cve_year_dir}\")\n",
    "        return []\n",
    "\n",
    "# Process target year\n",
    "cves_year = process_year_cves(year, 100000)\n",
    "\n",
    "print(f\"Total {year} CVEs: {len(cves_year)}\")\n",
    "\n",
    "# function for handling complex nested structure of containers column\n",
    "def to_json_safe(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, (str, int, float, bool)):\n",
    "        return v\n",
    "    try:\n",
    "        return json.dumps(v, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "# Serverless approach: Use pandas + createDataFrame instead of sparkContext\n",
    "    \n",
    "# Convert list of dicts to pandas DataFrame first\n",
    "print(f\"Converting {len(cves_year)} CVEs to DataFrame...\")\n",
    "\n",
    "pdf = pd.DataFrame(cves_year)\n",
    "pdf['containers'] = pdf['containers'].apply(to_json_safe)\n",
    "\n",
    "# Convert pandas to Spark DataFrame (serverless compatible)\n",
    "df_raw = spark.createDataFrame(pdf)\n",
    "\n",
    "print(f\"Total {year} CVE records found: {df_raw.count():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5575025b-2d8a-4c38-8390-d2d1420a5f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 38753 CVEs to DataFrame...\n2024 CVE records loaded: 38,753\n\nPerforming data quality checks...\nVerified 38,753 (>30,000) CVE records from 2024\nVerified no null CVE ids from 2024\nVerified 38753 unique CVE records from 2024\n\n2024 Bronze layer created: /Volumes/workspace/default/assignment1/cve/bronze\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Complete Bronze Layer Implementation\n",
    "# YOUR TASK: Add the missing Bronze layer components below\n",
    "\n",
    "# Optimize for Community Edition\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "\n",
    "# function for handling complex nested structure of containers column\n",
    "def to_json_safe(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, (str, int, float, bool)):\n",
    "        return v\n",
    "    try:\n",
    "        return json.dumps(v, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "\n",
    "def save_cves_to_delta_serverless(cves_list, year, delta_path):\n",
    "    \"\"\"Save CVEs to Delta Lake bronze layer - SERVERLESS COMPATIBLE\"\"\"\n",
    "    \n",
    "    if not cves_list:\n",
    "        print(f\"No CVEs to save for {year}\")\n",
    "        return\n",
    "    \n",
    "    # Serverless approach: Use pandas + createDataFrame instead of sparkContext\n",
    "\n",
    "    # 1) Read raw JSON recursively and add lineage\n",
    "    # Convert list of dicts to pandas DataFrame first\n",
    "    print(f\"Converting {len(cves_list)} CVEs to DataFrame...\")\n",
    "\n",
    "    pdf = pd.DataFrame(cves_list)\n",
    "    pdf['containers'] = pdf['containers'].apply(to_json_safe)\n",
    "\n",
    "    # Convert pandas to Spark DataFrame (serverless compatible)\n",
    "    df_raw = spark.createDataFrame(pdf)\n",
    "\n",
    "    # Add metadata columns\n",
    "    df_bronze = (df_raw\n",
    "                 .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "                 .withColumn(\"_ingestion_date\", current_date())\n",
    "                 .withColumn(\"_year\", lit(year))\n",
    "                 .withColumn(\"_record_id\", monotonically_increasing_id()))\n",
    "    \n",
    "    print(f\"{year} CVE records loaded: {df_bronze.count():,}\")\n",
    "\n",
    "    # 2) Add 2024 filtering logic\n",
    "    #Completed above while reading JSON files\n",
    "\n",
    "    # 3) Add data quality checks\n",
    "\n",
    "    # Data Quality Checks\n",
    "    print(\"\\nPerforming data quality checks...\")\n",
    "    record_count = df_bronze.count()\n",
    "    null_cve_ids = df_bronze.filter(col(\"_record_id\").isNull()).count()\n",
    "    unique_cve_ids = df_bronze.select(\"_record_id\").distinct().count()\n",
    "\n",
    "    assert record_count > 30000, f\"Warning: {record_count:,} (<30,000) CVE records from {year}\"\n",
    "    print(f\"Verified {record_count:,} (>30,000) CVE records from {year}\")\n",
    "        \n",
    "    assert null_cve_ids == 0, f\"Warning: {null_cve_ids} CVE records with null CVE id\"\n",
    "    print(f\"Verified no null CVE ids from {year}\")\n",
    "\n",
    "    assert unique_cve_ids == record_count, f\"Warning: {record_count - unique_cve_ids} CVE records with duplicate CVE ids\"\n",
    "    print(f\"Verified {unique_cve_ids} unique CVE records from {year}\")\n",
    "\n",
    "    # 4) Write Delta table\n",
    "    # Write to Delta Lake\n",
    "    (df_bronze.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "    .save(delta_path))\n",
    "    \n",
    "    print(f\"\\n{year} Bronze layer created: {delta_path}\")\n",
    "    return df_bronze\n",
    "\n",
    "# Save target year with serverless approach\n",
    "df_2024 = save_cves_to_delta_serverless(cves_year, year, DELTA_BRONZE_PATH)\n",
    "\n",
    "# Create schema if it does not exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.cve_bronze\")\n",
    "\n",
    "# 5) Register table for SQL access\n",
    "(df_2024.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "    .saveAsTable(\"cve_bronze.records\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f87a9342-aa3c-4461-8a49-8fd7ac714977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024 Bronze Layer Record Count: 38753\n\nroot\n |-- dataType: string (nullable = true)\n |-- dataVersion: string (nullable = true)\n |-- cveMetadata: struct (nullable = true)\n |    |-- assignerOrgId: string (nullable = true)\n |    |-- assignerShortName: string (nullable = true)\n |    |-- cveId: string (nullable = true)\n |    |-- datePublished: string (nullable = true)\n |    |-- dateRejected: string (nullable = true)\n |    |-- dateReserved: string (nullable = true)\n |    |-- dateUpdated: string (nullable = true)\n |    |-- state: string (nullable = true)\n |-- containers: string (nullable = true)\n |-- _ingestion_timestamp: timestamp (nullable = true)\n |-- _ingestion_date: date (nullable = true)\n |-- _year: integer (nullable = true)\n |-- _record_id: long (nullable = true)\n\n\n+------+------------------------------------+----------------------------+-----------+--------+-----------------------+-------------------+----------------+-----------------+--------+-----------+--------------------------------------------------------------------------------------------------------------+----------------+----------------+--------------------------------------------------------+---------------------------------------------------------------+-------------+\n|format|id                                  |name                        |description|location|createdAt              |lastModified       |partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties                                                                                                    |minReaderVersion|minWriterVersion|tableFeatures                                           |statistics                                                     |clusterByAuto|\n+------+------------------------------------+----------------------------+-----------+--------+-----------------------+-------------------+----------------+-----------------+--------+-----------+--------------------------------------------------------------------------------------------------------------+----------------+----------------+--------------------------------------------------------+---------------------------------------------------------------+-------------+\n|delta |08813f86-6a4e-4da5-99aa-e95fe7526562|workspace.cve_bronze.records|NULL       |        |2025-11-10 00:52:09.145|2025-11-16 22:34:42|[]              |[]               |8       |37879938   |{delta.columnMapping.mode -> name, delta.enableDeletionVectors -> true, delta.columnMapping.maxColumnId -> 16}|3               |7               |[appendOnly, columnMapping, deletionVectors, invariants]|{numRowsDeletedByDeletionVectors -> 0, numDeletionVectors -> 0}|false        |\n+------+------------------------------------+----------------------------+-----------+--------+-----------------------+-------------------+----------------+-----------------+--------+-----------+--------------------------------------------------------------------------------------------------------------+----------------+----------------+--------------------------------------------------------+---------------------------------------------------------------+-------------+\n\n\n\uD83D\uDCF8 REQUIRED SCREENSHOTS:\n   • df_2024.count() showing ~40,000 records\n   • DESCRIBE DETAIL cve_bronze.records output\n   • Data quality assertion results\n   • Delta files visible in path\n"
     ]
    }
   ],
   "source": [
    "# 6) Verification and screenshots\n",
    "\n",
    "# bronze record count\n",
    "print(f\"{year} Bronze Layer Record Count: {df_2024.count()}\\n\") \n",
    "\n",
    "# bronze schema\n",
    "spark.read.format(\"delta\").load(DELTA_BRONZE_PATH).printSchema()\n",
    "print()\n",
    "\n",
    "# bronze describe detail\n",
    "spark.sql(f\"DESCRIBE DETAIL {schema_name}.{table_name}\").show(truncate=False)\n",
    "\n",
    "print()\n",
    "print(\"\uD83D\uDCF8 REQUIRED SCREENSHOTS:\")\n",
    "print(\"   • df_2024.count() showing ~40,000 records\")\n",
    "print(\"   • DESCRIBE DETAIL cve_bronze.records output\")\n",
    "print(\"   • Data quality assertion results\")\n",
    "print(\"   • Delta files visible in path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbde4c32-fa57-4195-b4e4-e9cdbe6bc38a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning temporary files...\nTemporary files will be cleaned up automatically\n"
     ]
    }
   ],
   "source": [
    "# Cleanup temporary files\n",
    "print(\"Cleaning temporary files...\")\n",
    "\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(\"/tmp/cve_graph_demo\")\n",
    "    print(\"Temporary files cleaned\")\n",
    "except:\n",
    "    print(\"Temporary files will be cleaned up automatically\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6211854368548323,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_cvelist",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}